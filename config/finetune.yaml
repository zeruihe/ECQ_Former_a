# Fine-tuning config for Med-VQA (offline)

output:
  out_dir: "/root/autodl-tmp/outputs"
  run_name: "m1_finetune_pathvqa_clip"

models:
  # Offline cached model directories (already on your AutoDL disk)
  llama_dir: "/root/autodl-tmp/hf_models/TsinghuaC3I/Llama-3.1-8B-UltraMedical"
  clip_dir: "/root/autodl-tmp/hf_models/openai/clip-vit-large-patch14"
  dinov2_dir: "/root/autodl-tmp/hf_models/facebook/dinov2-large"
  biomedclip_dir: "/root/autodl-tmp/hf_models/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224"

  # Encoder ablation switch: [clip], [clip, biomedclip], [clip, biomedclip, dinov2]
  enabled_encoders: ["clip"]
  auto_m_queries: true   # m_queries = 32 * len(enabled_encoders)

  d_bridge: 768
  meq_layers: 12
  meq_heads: 12
  # m_queries: 96  # optional if auto_m_queries=false

data:
  dataset_id: "flaviagiammarino/path-vqa"   # or flaviagiammarino/vqa-rad, BoKelvin/SLAKE
  split: "train"                            # train / validation / test
  cache_dir: "/root/autodl-tmp/datasets/path-vqa"      # HF cache root (already downloaded)
  local_dir: null                            # optional: if you used save_to_disk
  images_dir: null                           # optional: set for SLAKE if images are extracted
  language: "en"                            # SLAKE supports en/zh; ignored for others

  batch_size: 2
  num_workers: 0
  pin_memory: true
  shuffle: true
  max_samples: null

train:
  seed: 42
  bf16: true
  fp16: false

  # Initialize from a pretrain checkpoint (trainable-only)
  init_from: "/root/autodl-tmp/outputs/m1_pertrain_clip/checkpoints"

  lr: 2.0e-5
  weight_decay: 0.0
  grad_accum: 4
  
  # Epoch-based training (recommended)
  # num_epochs takes priority over max_steps
  # Finetune: typically 3-10 epochs
  num_epochs: 3
  # max_steps: 2000  # fallback if num_epochs is not set
  
  max_length: 256
  log_every: 10
  save_every: 2000
  keep_last_k: 3
  save_latest: true

  # Prompt
  prompt_template: "You are a helpful medical assistant. Answer the question based on the image.\nQuestion: {question}\nAnswer:"

eval:
  enabled: true
  split: "validation"
  max_new_tokens: 10      # 限制生成长度，强制简短回答
  num_beams: 3            # 使用 beam search 提高质量
  temperature: 0.1        # 降低随机性
  top_p: 0.9